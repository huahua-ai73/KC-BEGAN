{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ca42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# import tensorflow.tensorflow_addons as tfa\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,LeakyReLU,Conv2DTranspose,Reshape,Conv2D,Dropout,Flatten,AveragePooling1D,Conv1D,MaxPooling1D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ffd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a5092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关数据\n",
    "os.chdir(r'D:\\回归模型所需改进')\n",
    "x_train = np.load('trainx.npy',allow_pickle=True).astype('float64').reshape(120,2500,1)\n",
    "y_train = np.load('trainy.npy',allow_pickle=True).astype('float64').reshape(120,1)\n",
    "x_test = np.load('testx.npy',allow_pickle=True).astype('float64').reshape(30,2500,1)\n",
    "y_test = np.load('testy.npy',allow_pickle=True).astype('float64').reshape(30,1)\n",
    "generated_x1 = np.load('XfromX1.npy', allow_pickle=True).astype('float64').reshape(120,2500,1)\n",
    "generated_x2 = np.load('XfromX2.npy', allow_pickle=True).astype('float64').reshape(120,2500,1)\n",
    "generated_x3 = np.load('XfromX3.npy', allow_pickle=True).astype('float64').reshape(120,2500,1)\n",
    "generated_y1 = np.load('YfromY1.npy',allow_pickle = True).reshape(120,1).astype('float64')\n",
    "generated_y2 = np.load('YfromY2.npy',allow_pickle = True).reshape(120,1).astype('float64')\n",
    "generated_y3 = np.load('YfromY3.npy',allow_pickle = True).reshape(120,1).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理导入数据\n",
    "# 自变量：X\n",
    "comX1 = np.vstack([x_train,generated_x1]).reshape(240,2500,1)#R+A\n",
    "comX2 = np.vstack([x_train,generated_x2]).reshape(240,2500,1)#R+B\n",
    "comX3 = np.vstack([x_train,generated_x3]).reshape(240,2500,1)#R+C\n",
    "comX4 = np.vstack([x_train,generated_x1,generated_x2]).reshape(360,2500,1)#R+A+B\n",
    "comX5 = np.vstack([x_train,generated_x1,generated_x3]).reshape(360,2500,1)#R+A+C\n",
    "comX6 = np.vstack([x_train,generated_x2,generated_x3]).reshape(360,2500,1)#R+B+C\n",
    "comX7 = np.vstack([x_train,generated_x1,generated_x2,generated_x3]).reshape(480,2500,1)#R+A+B+C\n",
    "# 因变量：y\n",
    "comY1 = np.vstack([y_train,generated_y1]).reshape(240,1).astype('float64')#R+A\n",
    "comY2 = np.vstack([y_train,generated_y2]).reshape(240,1).astype('float64')#R+B\n",
    "comY3 = np.vstack([y_train,generated_y3]).reshape(240,1).astype('float64')#R+C\n",
    "comY4 = np.vstack([y_train,generated_y1,generated_y2]).reshape(360,1).astype('float64')#R+A+B\n",
    "comY5 = np.vstack([y_train,generated_y1,generated_y3]).reshape(360,1).astype('float64')#R+A+C\n",
    "comY6 = np.vstack([y_train,generated_y2,generated_y3]).reshape(360,1).astype('float64')#R+B+C\n",
    "comY7 = np.vstack([y_train,generated_y1,generated_y2,generated_y3]).reshape(480,1).astype('float64')#R+A+B+C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ed0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加多尺度\n",
    "def Inception_ResNet():\n",
    "    # 定义一个神经网络\n",
    "    inputs = tf.keras.Input(shape=(2500,1))# 定义输入层\n",
    "    \n",
    "    c1 = tf.keras.layers.Conv1D(16,3, padding='same')(inputs)# 尺度一\n",
    "    c1 = tf.keras.activations.gelu(c1)\n",
    "    \n",
    "    c2 = tf.keras.layers.Conv1D(16,3, padding='same')(c1)\n",
    "    c2 = tf.keras.activations.gelu(c2)\n",
    "    \n",
    "    d1 = tf.keras.layers.Conv1D(16,5, padding='same')(inputs)# 尺度二\n",
    "    d1 = tf.keras.activations.gelu(d1)\n",
    "    \n",
    "    d2 = tf.keras.layers.Conv1D(16,5, padding='same')(d1)\n",
    "    d2 = tf.keras.activations.gelu(d2)\n",
    "    \n",
    "    add1 = tf.keras.layers.add([c1, d1])\n",
    "    p1 = tf.keras.layers.MaxPool1D(3)(add1)# 最大池化层\n",
    "    \n",
    "    c3 = tf.keras.layers.Conv1D(64,5, padding='same')(p1)\n",
    "    c3 = tf.keras.activations.gelu(c3)\n",
    "    \n",
    "    c4 = tf.keras.layers.Conv1D(64,5, padding='same')(c3)\n",
    "    c4 = tf.keras.activations.gelu(c4)\n",
    "    \n",
    "    d3 = tf.keras.layers.Conv1D(64,3, padding='same')(p1)\n",
    "    d3 = tf.keras.activations.gelu(d3)\n",
    "    \n",
    "    d4 = tf.keras.layers.Conv1D(64,3, padding='same')(d3)\n",
    "    d4 = tf.keras.activations.gelu(d4)\n",
    "    \n",
    "    add2 = tf.keras.layers.add([c4, d4])\n",
    "    p2 = tf.keras.layers.MaxPool1D(3)(add2)\n",
    "    \n",
    "    c5 = tf.keras.layers.Conv1D(128,3, padding='same')(p2)\n",
    "    c5 = tf.keras.activations.gelu(c5)\n",
    "    \n",
    "    c6 = tf.keras.layers.Conv1D(128,3, padding='same')(c5)\n",
    "    c6 = tf.keras.activations.gelu(c6)\n",
    "    \n",
    "    d5 = tf.keras.layers.Conv1D(128,5, padding='same')(p2)\n",
    "    d5 = tf.keras.activations.gelu(d5)\n",
    "    \n",
    "    d6 = tf.keras.layers.Conv1D(128,5, padding='same')(d5)\n",
    "    d6 = tf.keras.activations.gelu(d6)\n",
    "    \n",
    "    add3 = tf.keras.layers.add([c6, d6])\n",
    "    p3 = tf.keras.layers.MaxPool1D(3)(add3)\n",
    "    \n",
    "    c7 = tf.keras.layers.Conv1D(16,5, padding='same')(p3)\n",
    "    c7 = tf.keras.activations.gelu(c7)\n",
    "    \n",
    "    c8 = tf.keras.layers.Conv1D(16,5, padding='same')(c7)\n",
    "    c8 = tf.keras.activations.gelu(c8)\n",
    "    \n",
    "    d7 = tf.keras.layers.Conv1D(16,3, padding='same')(p3)\n",
    "    d7 = tf.keras.activations.gelu(d7)\n",
    "    \n",
    "    d8 = tf.keras.layers.Conv1D(16,3, padding='same')(d7)\n",
    "    d8 = tf.keras.activations.gelu(d8)\n",
    "    \n",
    "    add4 = tf.keras.layers.add([c8, d8])\n",
    "    p4 = tf.keras.layers.MaxPool1D(5)(add4)\n",
    "    \n",
    "    f1 = tf.keras.layers.Flatten()(p4)# 拉伸层\n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(dropout)# 全连接层\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model \n",
    "model = Inception_ResNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义损失函数\n",
    "def MSE(x_batch, y_batch):# 输入的真实x，y\n",
    "    y_pred = model(x_batch)\n",
    "    y_pred = tf.cast(y_pred,dtype=tf.float64)\n",
    "    y_batch = tf.cast(y_batch,dtype=tf.float64)\n",
    "    mse_loss = tf.reduce_mean(tf.square(y_pred-y_batch))\n",
    "    mse_loss = tf.cast(mse_loss,dtype=tf.float64)\n",
    "    return mse_loss\n",
    "def MSE1(x_batch, y_batch):# 输入的真实x，y\n",
    "    y_pred = model(x_batch, training=False)\n",
    "    y_pred = tf.cast(y_pred,dtype=tf.float64)\n",
    "    y_batch = tf.cast(y_batch,dtype=tf.float64)\n",
    "    mse_loss = tf.reduce_mean(tf.square(y_pred-y_batch))\n",
    "    mse_loss = tf.cast(mse_loss,dtype=tf.float64)\n",
    "    return mse_loss\n",
    "def r_2(x_batch, y_batch):\n",
    "    y_pred = model(x_batch)\n",
    "    y_pred = tf.cast(y_pred,dtype=tf.float64)\n",
    "    y_batch = tf.cast(y_batch,dtype=tf.float64)\n",
    "    y_mean = tf.reduce_mean(y_batch)\n",
    "    SSE = tf.reduce_sum(tf.square(y_batch-y_pred))    \n",
    "    SST = tf.reduce_sum(tf.square(y_batch-y_mean))  \n",
    "    R2 = 1 - SSE/SST\n",
    "    return R2\n",
    "model_optimizer1 = tf.optimizers.Adam(0.001)\n",
    "model_optimizer2 = tf.optimizers.Adam(0.0005)\n",
    "model_optimizer2 = tf.optimizers.Adam(0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40207210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义早停法一\n",
    "def stop_train(patience, score, epoch, num_patience,i):\n",
    "    global best_score\n",
    "    if epoch == 0:# 记录最好的结果\n",
    "        best_score = score\n",
    "        print('更新了历史最优MSE为:%s'%best_score)\n",
    "    elif score >= best_score:# 如果模型没有进步的话,这次的MSE应该高于历史最低MSE\n",
    "        #将迭代次数增1\n",
    "        num_patience = num_patience+1\n",
    "        print('MSE:容忍度更新为：%s'%num_patience)\n",
    "        print(\"此次训练未成功优化性能，历史最优MSE仍为：%s\"%best_score)\n",
    "        print(\"此次训练未成功优化性能，本次MSE为：%s\"%score)\n",
    "        if num_patience >= patience:# 大于容忍度，模型训练暂停\n",
    "            return -1\n",
    "    else:\n",
    "        best_score = score# 更新历史最优MSE\n",
    "        num_patience = 0 # 重置容忍度\n",
    "        print(\"//**********************************//\")\n",
    "        print('更新了历史最优MSE为:%s'%best_score)\n",
    "    model.save('model{}.h5'.format(i))\n",
    "    return num_patience\n",
    "# 定义早停法二\n",
    "def stop_train_r2(patience, input_r2, epoch, r2_patience,i):\n",
    "    global best_r2\n",
    "    if epoch == 0:# 记录最好的结果\n",
    "        best_r2 = input_r2\n",
    "        print('更新了历史最优测试集R2为:%s'%best_score)\n",
    "    elif input_r2 <= best_r2:# 如果模型没有进步的话,这次的MSE应该高于历史最低MSE\n",
    "        #将迭代次数增1\n",
    "        r2_patience = r2_patience+1\n",
    "        print('R2:容忍度更新为：%s'%r2_patience)\n",
    "        print(\"此次训练未成功优化性能，历史最优测试集R2仍为：%s\"%best_r2)\n",
    "        print(\"此次训练未成功优化性能，本次测试集R2为：%s\"%input_r2)\n",
    "        if r2_patience >= patience:# 大于容忍度，模型训练暂停\n",
    "            return -1\n",
    "    else:\n",
    "        best_r2 = input_r2# 更新历史最优MSE\n",
    "        r2_patience = 0 # 重置容忍度\n",
    "        print(\"//**********************************//\")\n",
    "        print('更新了历史最优测试集R2为:%s'%best_r2)\n",
    "    return r2_patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5250dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetX = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(80).batch(40)\n",
    "dataSetXA = tf.data.Dataset.from_tensor_slices((comX1, comY1)).shuffle(160).batch(80)\n",
    "dataSetXB = tf.data.Dataset.from_tensor_slices((comX2, comY2)).shuffle(160).batch(80)\n",
    "dataSetXC = tf.data.Dataset.from_tensor_slices((comX3, comY3)).shuffle(160).batch(80)\n",
    "dataSetXAB = tf.data.Dataset.from_tensor_slices((comX4, comY4)).shuffle(240).batch(120)\n",
    "dataSetXAC = tf.data.Dataset.from_tensor_slices((comX5, comY5)).shuffle(240).batch(120)\n",
    "dataSetXBC = tf.data.Dataset.from_tensor_slices((comX6, comY6)).shuffle(240).batch(120)\n",
    "dataSetXABC = tf.data.Dataset.from_tensor_slices((comX7, comY7)).shuffle(320).batch(160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4ca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r2 = []\n",
    "train_r2 = []\n",
    "def train(dataset, epochs, trainxFor_thisepoch, trainyFor_thisepoch):\n",
    "    i = 0\n",
    "    num_patience = 0\n",
    "    r2_patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        if num_patience == -1 or r2_patience == -1:\n",
    "            break\n",
    "        if epoch < 400:\n",
    "            for xBatch, yBatch in dataset:\n",
    "                with tf.GradientTape() as regressionTape:\n",
    "                    loss = MSE(xBatch, yBatch)\n",
    "                gradients_of_regression = regressionTape.gradient(loss, model.trainable_variables)\n",
    "                model_optimizer1.apply_gradients(zip(gradients_of_regression, model.trainable_variables))# 更新权值\n",
    "                loss = MSE1(xBatch, yBatch)# 这里设置的应该是训练集整体\n",
    "                score = loss\n",
    "                r1 = r_2(trainxFor_thisepoch, trainyFor_thisepoch)# 这里设置的应该是训练集整体\n",
    "                r2 = r_2(x_test, y_test)# 这里设置的应该是训练集整体 \n",
    "                test_r2.append(r2)\n",
    "                train_r2.append(r1)\n",
    "                num_patience = stop_train(300, score, epoch, num_patience,i)\n",
    "                r2_patience = stop_train_r2(300, r2, epoch, r2_patience,i)\n",
    "                i = i+1\n",
    "            if num_patience == -1 or r2_patience == -1:\n",
    "                break\n",
    "        if 800>epoch >=400:\n",
    "            if epoch==400:\n",
    "                num_patience = 0\n",
    "                r2_patience = 0\n",
    "            for xBatch, yBatch in dataset:\n",
    "                with tf.GradientTape() as regressionTape:\n",
    "                    loss = MSE(xBatch, yBatch)\n",
    "                gradients_of_regression = regressionTape.gradient(loss, model.trainable_variables)\n",
    "                model_optimizer2.apply_gradients(zip(gradients_of_regression, model.trainable_variables))# 更新权值(更换学习率)\n",
    "                loss = MSE1(xBatch, yBatch)# 这里设置的应该是训练集整体\n",
    "                score = loss\n",
    "                r1 = r_2(trainxFor_thisepoch, trainyFor_thisepoch)# 这里设置的应该是训练集整体\n",
    "                r2 = r_2(x_test, y_test)# 这里设置的应该是训练集整体 \n",
    "                test_r2.append(r2)\n",
    "                train_r2.append(r1)\n",
    "                num_patience = stop_train(300, score, epoch, num_patience,i)\n",
    "                r2_patience = stop_train_r2(300, r2, epoch, r2_patience,i)\n",
    "                i = i+1\n",
    "            if num_patience == -1 or r2_patience == -1:\n",
    "                break\n",
    "        if epoch >=800:\n",
    "            if epoch==800:\n",
    "                num_patience = 0\n",
    "                r2_patience = 0\n",
    "            for xBatch, yBatch in dataset:\n",
    "                with tf.GradientTape() as regressionTape:\n",
    "                    loss = MSE(xBatch, yBatch)\n",
    "                gradients_of_regression = regressionTape.gradient(loss, model.trainable_variables)\n",
    "                model_optimizer3.apply_gradients(zip(gradients_of_regression, model.trainable_variables))# 更新权值(更换学习率)\n",
    "                loss = MSE1(xBatch, yBatch)# 这里设置的应该是训练集整体\n",
    "                score = loss\n",
    "                r1 = r_2(trainxFor_thisepoch, trainyFor_thisepoch)# 这里设置的应该是训练集整体\n",
    "                r2 = r_2(x_test, y_test)# 这里设置的应该是训练集整体 \n",
    "                test_r2.append(r2)\n",
    "                train_r2.append(r1)\n",
    "                num_patience = stop_train(120, score, epoch, num_patience,i)\n",
    "                r2_patience = stop_train_r2(120, r2, epoch, r2_patience,i)\n",
    "                i = i+1\n",
    "            if num_patience == -1 or r2_patience == -1:\n",
    "                break\n",
    "        print('这是第%s次训练'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5d5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# 模型训练 \n",
    "#记录运行时间\n",
    "os.chdir(r'D:/Inception/X')# 需要更改\n",
    "with tf.device('/GPU:0'):\n",
    "    start = time.perf_counter()\n",
    "    train(dataSetX, 4000, x_train, y_train)# 需要更改\n",
    "    end = time.perf_counter()\n",
    "    print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r2 = np.array(test_r2)\n",
    "train_r2 = np.array(train_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Test_r2.npy\",test_r2)\n",
    "np.save(\"Train_r2.npy\",train_r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
